\subsection{User Study}\label{sec:user}


\if 0
\begin{figure*}
     \centering
     \begin{tabular}{ccc}
		\includegraphics[width=0.22\linewidth]{pictures/user_study/quality}
		&
		\includegraphics[width=0.22\linewidth]{pictures/user_study/clutter}
        &
        \includegraphics[width=0.22\linewidth]{pictures/user_study/accept}
		\\
		(A) T1: visual quality study
		&
		(B) T2: visual clutter study
        &
        (C) T3: acceptable visualization study
	\end{tabular}
	\vspace{-4mm}
	\caption{User study results of different visualization methods}
	\label{fig:userstudy}
    \trim
\end{figure*}
\fi

In this part, we conduct a user study to evaluate the quality of the visualizations generated by different methods objectively.


%We recruited 35 participants with 10 females, 25 males, aged 19 to 31 with a mean of 24.78 for the user study. 
%The user study is conducted on the \pt{} and \sz{} datasets, and four visualization methods are investigated, i.e., $\full$, $\rand$, $\mathsf{DTW}$ and $\avats$. 
%We manually select 22 \textit{center points} in the two datasets and define 3 \textit{visualization scales} including: large-scale region (with zoom level smaller than 13), middle-scale region (with zoom level between 13 and 15), small-scale region (with zoom level larger than 15). For each center point and at each visualization scale, we generate a \textit{comparable visualization group}, which includes one visualization generated by each of the 4 methods. This results in 66 comparable groups (22 center points $\times$ 3 scales) and 264 visualization results (66 comparable groups $\times$ 4 visualizations).

\stitle{Settings} We recruited 35 participants with 10 females, 25 males, aged 19 to 31 with a mean of 24.78 for the user study. The user study is conducted on the \pt{} and \sz{} datasets, and four visualization methods are investigated, i.e., $\full$, $\rand$, $\mathsf{DTW}$ and $\avats$. We define a \textit{comparable group} as the set of visualizations generated by different methods with the same center point and zoom scale. We manually generated 66 comparable groups and 264 visualizations results (66 groups $\times$ 4 visualizations). The user study system is a web-based platform, in which all visualizations are displayed with a resolution of 450*300. We are interested in the visual quality and visual clutter of the visualizations, and thus designed three tasks for a comparable group:
\begin{itemize}
	\item Task 1 (T1): rank the visualizations in a group from the highest visual quality to the lowest visual quality. %by 1st, 2nd, 3rd, and 4th.
	\item Task 2 (T2): rank the visualizations in a group from the least visual clutter to the most severe visual clutter. %by 1st, 2nd, 3rd, and 4th.
	\item Task 3 (T3): select the visualizations considered acceptable (multiple choices allowed) and choose the reason (including ``severe visual clutter", ``poor visual quality" and ``others") for the visualizations considered unacceptable. 
\end{itemize}


\stitle{User study procedure} 
When the participants enter the user study system, they are given a tutorial on how to conduct the tasks to get familiar with the interface and tasks.
For each participant, we randomly select 16 comparable groups.
%Thus, we obtain $35 \times 16 = 560$ results for each task.
For each comparable group, the 4 visualizations (\textit{without specifying generated by which method}) in it are displayed on the same page and a participant is required to perform task T1, T2 and T3 by inspecting them.


%The results show that $\full$ ranks the 1st in most cases while $\avats$ usually ranks 1st or 2nd. i.e., the percentage of $\avats$ ranks top-2 among 4 methods is $88.9\%$. 
%In contrast, $\mathsf{DTW}$ and $\rand$ rank 3rd and 4th at most times. 
%This suggests that the visualizations generated by $\avats$ are more appealing to the participants than $\mathsf{DTW}$ and $\rand$. 
%We also observed that the participants tend to rank $\avats$ before $\full$ for large-scale regions with  many trajectories, and the other way for smaller regions. 
\stitle{Result analysis} 
Figure~\ref{fig:userstudy}(A) reports the visual quality ranking of the 4 methods in T1. The results show that $\full$ ranks 1st in most cases while $\avats$ usually ranks 1st or 2nd. 
In contrast, $\mathsf{DTW}$ and $\rand$ rank 3rd and 4th at most times. 
This suggests that the visualizations generated by $\avats$ are more appealing to the participants than $\mathsf{DTW}$ and $\rand$. Figure~\ref{fig:userstudy}(B) reports the anti-visual clutter ranking of the 4 methods in T2. 
The results show that visual clutter is most severe for $\full$, ranking 4th in most cases.    
While $\avats$ is the most successful in reducing visual clutter, ranking 1st in 255 out of the 560 cases and ranking 4th for only 19 cases.
We report the acceptance frequency of the 4 methods in T3 using bar chart in Figure~\ref{fig:userstudy}(C). 
Each column corresponds to a method, and from bottom to top, the lengths of the bars indicate the percentage of participants choosing ``acceptable'', ``not acceptable due to visual clutter'', ``not acceptable due to poor visual quality'' and ``not acceptable for other reasons''. 
The results show that $\avats$ is regarded acceptable for about 88.2\% of the cases, and the other methods have much lower acceptance rate than $\avats$.   

%The time usage for T1 range from 816  seconds to 5748 seconds with a mean of 2320 seconds.
%We also observed that the participants tend to rank $\avats$ before $\full$ for large-scale regions with  many trajectories, and the other way for smaller regions. 

%Figure~\ref{fig:userstudy}(B) reports the anti-visual clutter ranking of the 4 methods in T2. 
%The results show that visual clutter is most severe for $\full$, ranking 4th in most cases (349 over 560). With sampling, $\mathsf{DTW}$ usually rank 2nd and 3rd but $\rand$ ranks 4th for a considerable number of times as it tends to create clutter in the dense regions.    
%$\avats$ is the most successful in reducing visual clutter, ranking 1st in 255 out of the 560 cases and ranking 4th for only 19 cases.    

%\QM{Figure~\ref{fig:userstudy}(B) reports the anti-visual clutter ranking of the 4 methods in T2. 
	%The results show that visual clutter is most severe for $\full$, ranking 4th in most cases.    
	%While $\avats$ is the most successful in reducing visual clutter, ranking 1st in 255 out of the 560 cases and ranking 4th for only 19 cases. 
	%The time usage for T2 range from 513 seconds to 9503 seconds with a mean of 2320 seconds.}

%We report the frequency each of the 4 methods is selected as acceptable and why a method is not selected in T3 using bar chart in Figure~\ref{fig:userstudy}(C). 
%Each column corresponds to a method, and from bottom to top, the lengths of the bars indicate the percentage of participants choosing ``acceptable'', ``not acceptable due to visual clutter'', ``not acceptable due to poor visual quality'' and ``not acceptable for other reasons''. 
%The results show that $\avats$ is regarded acceptable for about 88.2\% of the cases, and the other methods have significantly lower acceptance rate than $\avats$. 
%Specifically, $\mathsf{DTW}$ and $\rand$ have low acceptance rate mainly due to poor visual quality while $\full$ suffers from severe visual clutter.

%\QM{We report the acceptance frequency of the 4 methods in T3 using bar chart in Figure~\ref{fig:userstudy}(C). 
	%Each column corresponds to a method, and from bottom to top, the lengths of the bars indicate the percentage of participants choosing ``acceptable'', ``not acceptable due to visual clutter'', ``not acceptable due to poor visual quality'' and ``not acceptable for other reasons''. 
	%The results show that $\avats$ is regarded acceptable for about 88.2\% of the cases, and the other methods have significantly lower acceptance rate than $\avats$. The time usage for T3 range from 539 seconds to 1334 seconds with a mean of 1018 seconds.
	%Specifically, $\mathsf{DTW}$ and $\rand$ have low acceptance rate mainly due to poor visual quality while $\full$ suffers from severe visual clutter.
%}



